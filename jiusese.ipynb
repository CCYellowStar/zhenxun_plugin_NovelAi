{"cells":[{"cell_type":"markdown","metadata":{"id":"OHCtG9MT0jLq"},"source":["Clone webui repository"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBbcB4vwj_jm","outputId":"10ee7663-9122-483e-e822-472ebea3cf71","executionInfo":{"status":"ok","timestamp":1666617294509,"user_tz":-480,"elapsed":9421,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'stable-diffusion-webui-api'...\n","remote: Enumerating objects: 8003, done.\u001b[K\n","remote: Total 8003 (delta 0), reused 0 (delta 0), pack-reused 8003\u001b[K\n","Receiving objects: 100% (8003/8003), 32.09 MiB | 26.54 MiB/s, done.\n","Resolving deltas: 100% (5622/5622), done.\n","[Errno 2] No such file or directory: 'stable-diffusion-webui'\n","/content/stable-diffusion-webui\n"]}],"source":["!git clone https://github.com/CCYellowStar/stable-diffusion-webui-api/\n","%cd stable-diffusion-webui-api"]},{"cell_type":"markdown","metadata":{"id":"F0EINk5M0s-w"},"source":["Download the model from NovelAILeaks."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzRNMT42Gw_p","outputId":"fe7ee7ce-9f07-4654-cb7c-5394d121bc50"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/stable-diffusion-webui/models/Stable-diffusion\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 4067M  100 4067M    0     0   171M      0  0:00:23  0:00:23 --:--:--  191M\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  784M  100  784M    0     0   168M      0  0:00:04  0:00:04 --:--:--  168M\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 1004M  100 1004M    0     0   130M      0  0:00:07  0:00:07 --:--:--  167M\n"]}],"source":["!mkdir -p /content/stable-diffusion-webui-api/models/Stable-diffusion\n","%cd /content/stable-diffusion-webui-api/models/Stable-diffusion/\n","\n","# 7G animefull-final-latest (may not work)\n","# !gdown 17WWd6KEsBj7D_0TyGp8aXHQDlchYVs1a -O /content/stable-diffusion-webui/models/Stable-diffusion/model.ckpt\n","\n","# 4G animefull-final-pruned\n","# !gdown 1d3f2fvN2gLRocBahZrXe_v1EEHuqpUzT -O /content/stable-diffusion-webui/models/Stable-diffusion/model.ckpt\n","\n","# 4G animefull-final-pruned (backup)\n","!curl -Lo model.ckpt https://cloudflare-ipfs.com/ipfs/bafybeicpamreyp2bsocyk3hpxr7ixb2g2rnrequub3j2ahrkdxbvfbvjc4/model.ckpt\n","\n","# Install VAE (optional)\n","!curl -Lo model.vae.pt https://cloudflare-ipfs.com/ipfs/bafybeiccldswdd3wvg57jhclcq53lvsc6gizasiblwayvhlv6eq4wow7wu/animevae.pt \n","\n","# Install embeddings (optional)\n","!curl -L https://cloudflare-ipfs.com/ipfs/bafybeie3hdjchxs5tz4n75bos53nhcklslguxchdurc2ynrzcfv2kwyklu/embeddings.tar | tar x -C /content/stable-diffusion-webui-api/embeddings"]},{"cell_type":"markdown","metadata":{"id":"xt8lbdmC04ox"},"source":["Launch web ui. You will get a link to nnn.gradio.app, follow it.\n","\n","Commandline arguments are:\n","  - `--share` - create online gradio.app link\n","  - `--gradio-debug` - print outputs to console\n","  - `--gradio-auth me:qwerty` - add authentication to gradio: username me, password qwerty"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-xAdMA5wxXd","outputId":"e412c63c-546b-4da4-87e6-70f173c16a5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/stable-diffusion-webui\n","Python 3.7.14 (default, Sep  8 2022, 00:06:44) \n","[GCC 7.5.0]\n","Commit hash: 4999eb2ef9b30e8c42ca7e4a94d4bbffe4d1f015\n","Installing gfpgan\n","Installing clip\n","Cloning Stable Diffusion into repositories/stable-diffusion...\n","Cloning Taming Transformers into repositories/taming-transformers...\n","Cloning K-diffusion into repositories/k-diffusion...\n","Cloning CodeFormer into repositories/CodeFormer...\n","Cloning BLIP into repositories/BLIP...\n","Installing requirements for CodeFormer\n","Installing requirements for Web UI\n","Launching Web UI with arguments: --share --gradio-debug\n","Warning: caught exception 'No CUDA GPUs are available', memory monitor disabled\n","LatentDiffusion: Running in eps-prediction mode\n","DiffusionWrapper has 859.52 M params.\n","making attention of type 'vanilla' with 512 in_channels\n","Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n","making attention of type 'vanilla' with 512 in_channels\n","Downloading: 100% 939k/939k [00:00<00:00, 8.24MB/s]\n","Downloading: 100% 512k/512k [00:00<00:00, 5.46MB/s]\n","Downloading: 100% 389/389 [00:00<00:00, 302kB/s]\n","Downloading: 100% 905/905 [00:00<00:00, 675kB/s]\n","Downloading: 100% 4.41k/4.41k [00:00<00:00, 3.16MB/s]\n","Downloading: 100% 1.59G/1.59G [00:33<00:00, 50.7MB/s]\n","Loading weights [925997e9] from /content/stable-diffusion-webui/models/Stable-diffusion/model.ckpt\n","Loading VAE weights from: /content/stable-diffusion-webui/models/Stable-diffusion/model.vae.pt\n","Model loaded.\n","Loaded a total of 12 textual inversion embeddings.\n","Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://23324.gradio.app\n","\n","This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/gradio/routes.py\", line 274, in run_predict\n","    fn_index, raw_input, username, session_state, iterators\n","  File \"/usr/local/lib/python3.7/dist-packages/gradio/blocks.py\", line 742, in process_api\n","    result = await self.call_function(fn_index, inputs, iterator)\n","  File \"/usr/local/lib/python3.7/dist-packages/gradio/blocks.py\", line 654, in call_function\n","    block_fn.fn, *processed_input, limiter=self.limiter\n","  File \"/usr/local/lib/python3.7/dist-packages/anyio/to_thread.py\", line 32, in run_sync\n","    func, *args, cancellable=cancellable, limiter=limiter\n","  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n","    result = context.run(func, *args)\n","  File \"/content/stable-diffusion-webui/modules/ui.py\", line 382, in update_token_counter\n","    tokens, token_count, max_length = max([model_hijack.tokenize(prompt) for prompt in prompts], key=lambda args: args[1])\n","  File \"/content/stable-diffusion-webui/modules/ui.py\", line 382, in <listcomp>\n","    tokens, token_count, max_length = max([model_hijack.tokenize(prompt) for prompt in prompts], key=lambda args: args[1])\n","  File \"/content/stable-diffusion-webui/modules/sd_hijack.py\", line 92, in tokenize\n","    max_length = opts.max_prompt_tokens - 2\n","  File \"/content/stable-diffusion-webui/modules/shared.py\", line 284, in __getattr__\n","    return super(Options, self).__getattribute__(item)\n","AttributeError: 'Options' object has no attribute 'max_prompt_tokens'\n","Error completing request\n","Arguments: ('extremely detailed CG, karin (blue archive) , 1girl, 1boy, dark skin, dark skin,dark skin, long_hair, large_breasts,  yellow_eyes, black_hair,  blush, looking_at_viewer, open_mouth, bangs, nipples, thighhighs, navel, nude, penis, pussy, collarbone, sweat, sex,  missionary, cumshot, cum on breasts, used condom,  hetero, lying, spread_legs, vaginal, parted_lips, on_back, white_gloves, bed, collar, white_thighhighs, arms_up, pillow, bdsm, maid_headdress, on_bed, slave, unaligned_breasts,nsfw', 'lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, bad feet', 'None', 'None', 20, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, False, 0.7, 0, False, False, None, '', 1, '', 4, '', True, False) {}\n","Traceback (most recent call last):\n","  File \"/content/stable-diffusion-webui/modules/ui.py\", line 158, in f\n","    res = list(func(*args, **kwargs))\n","  File \"/content/stable-diffusion-webui/webui.py\", line 66, in f\n","    res = func(*args, **kwargs)\n","  File \"/content/stable-diffusion-webui/modules/txt2img.py\", line 43, in txt2img\n","    processed = process_images(p)\n","  File \"/content/stable-diffusion-webui/modules/processing.py\", line 369, in process_images\n","    uc = prompt_parser.get_learned_conditioning(shared.sd_model, len(prompts) * [p.negative_prompt], p.steps)\n","  File \"/content/stable-diffusion-webui/modules/prompt_parser.py\", line 133, in get_learned_conditioning\n","    conds = model.get_learned_conditioning(texts)\n","  File \"/content/stable-diffusion-webui/repositories/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 558, in get_learned_conditioning\n","    c = self.cond_stage_model(c)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/stable-diffusion-webui/modules/sd_hijack.py\", line 272, in forward\n","    outputs = self.wrapped.transformer(input_ids=tokens, position_ids=position_ids)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/clip/modeling_clip.py\", line 728, in forward\n","    return_dict=return_dict,\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/clip/modeling_clip.py\", line 649, in forward\n","    return_dict=return_dict,\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/clip/modeling_clip.py\", line 578, in forward\n","    output_attentions=output_attentions,\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/clip/modeling_clip.py\", line 316, in forward\n","    hidden_states = self.layer_norm1(hidden_states)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py\", line 190, in forward\n","    input, self.normalized_shape, self.weight, self.bias, self.eps)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 2503, in layer_norm\n","    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n","RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'\n","\n","Interrupted with signal 2 in <frame at 0x7fc566174230, file '/usr/local/lib/python3.7/dist-packages/gradio/blocks.py', line 1238, code block_thread>\n"]}],"source":["%cd /content/stable-diffusion-webui-api\n","!COMMANDLINE_ARGS=\"--share --gradio-debug --api\" REQS_FILE=\"requirements.txt\" python launch.py"]},{"cell_type":"markdown","metadata":{"id":"T3957x2AY7iP"},"source":["commands for ***after*** you have gotten done with a session\n","============================================================================"]},{"cell_type":"markdown","metadata":{"id":"aCtJffM2ZE06"},"source":["Zip images for downloading on local drive (click the folder icon on the left, the one below {x})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmRqNyiAZCHu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"09d79d8c-dcdf-496b-c4b3-26bbda41c36a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\tzip warning: name not matched: /content/stable-diffusion-webui/outputs\n","\n","zip error: Nothing to do! (try: zip -r /content/stable-diffusion-webui . -i /content/stable-diffusion-webui/outputs)\n"]}],"source":["  !zip -r /content/stable-diffusion-webui-api /content/stable-diffusion-webui-api/outputs "]},{"cell_type":"markdown","metadata":{"id":"tcslU-S2ZNr9"},"source":["Save images to Google Drive **Warning: this will cause google to scan your drive, so if you intend to use this and worry about that kind of stuff, probablly just set this up on a clean account that's just for this colab**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-N0lnu-TZOTW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5c5c6d9-9595-42a0-840d-10804087e100"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","cp: cannot stat '/content/stable-diffusion-webui/outputs': No such file or directory\n"]}],"source":["from google.colab import drive # type: ignore\n","\n","try:\n","   drive_path = \"/content/drive\"\n","   drive.mount(drive_path,force_remount=False)\n","except:\n","   print(\"...error mounting drive or with drive path variables\")\n","\n","!cp -r \"/content/stable-diffusion-webui-api/outputs\" \"/content/drive/MyDrive\""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"https://gist.github.com/CCYellowStar/264846ef6f3dd2221e657b6c9b2bfa60#file--ipynb","timestamp":1666617448828}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}